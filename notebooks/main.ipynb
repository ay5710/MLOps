{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ebacec-5471-4a4d-8ab1-d1494a2652c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import s3fs\n",
    "import tqdm\n",
    "\n",
    "from datetime import datetime\n",
    "from src.analysis import GPT\n",
    "from src.scrapping import IMDb\n",
    "from src.utils.db import PostgreSQLDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ff5a5-90e0-4fa2-a6d2-0535de5c87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = PostgreSQLDatabase()\n",
    "db.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f3e87-c14f-4cad-9383-60fd14870744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_id = '0219822'  # Human nature (good for test, few reviews, some with spoilers) => already in the sample file\n",
    "# movie_id = '0089885'  # Re-animator\n",
    "# movie_id = '0101414'  # Beauty and the Best\n",
    "# movie_id = '0029583'  # Snow White (1937)\n",
    "# movie_id = '6208148'  # Snow White (2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c4f42e-fc56-4611-a876-5df1780ce14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "###          SCRAPPING         ###\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa75b7-21ad-4e96-bf34-a2cf1d34d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie_id in set(movie[0] for movie in db.query_data('movies')):\n",
    "    print(f\"[INFO] Begining scrapping for movie #{movie_id}\")\n",
    "\n",
    "    ###   Scrap movie metadata   ###\n",
    "    \n",
    "    scrapper = IMDb()\n",
    "    movie_scrap_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    movie_title, release_date = scrapper.get_movie(movie_id)\n",
    "    total_reviews = scrapper.get_number_of_reviews(movie_id)\n",
    "\n",
    "    # Update table (data must be passed as a list of tuples)\n",
    "    movie_data = [(movie_id, movie_title, release_date, total_reviews, movie_scrap_time)]\n",
    "    last_scrapping = db.query_data(\"movies\", condition=f\"movie_id = '{(movie_id)}'\")[0][4]\n",
    "    if last_scrapping is None:\n",
    "        db.remove_data(\"movies\", \"movie_id\", movie_id)\n",
    "        db.insert_data(\"movies\", movie_data)\n",
    "    else:\n",
    "        db.upsert_movie_data(movie_data)\n",
    "\n",
    "    ###   Scrap reviews   ###\n",
    "    \n",
    "    # Check if new reviews have been published or if the last scrapping is >24h old\n",
    "    old_total_reviews = db.query_data(\"movies\", condition=f\"movie_id = '{movie_id}'\")[0][3]\n",
    "    old_total_reviews = int(old_total_reviews) if old_total_reviews is not None else 0\n",
    "    new_reviews = total_reviews - old_total_reviews\n",
    "\n",
    "    last_scrapping = db.query_data(\"movies\", condition=f\"movie_id = '{(movie_id)}'\")[0][4]\n",
    "    if last_scrapping is None:\n",
    "        time_since_scrapping = float('inf')\n",
    "        prompt = f\"without\" if new_reviews==0 else f\"with\"\n",
    "        print(f\"[INFO] New movie {prompt} reviews to scrap!\")\n",
    "    else:\n",
    "        time_since_scrapping = (datetime.now() - last_scrapping).seconds\n",
    "        prompt = f\"No new review\" if new_reviews==0 else f\"{new_reviews} new reviews\"\n",
    "        print(f\"[INFO] {prompt} published in the last {time_since_scrapping / 3600:.2F} hours\")\n",
    "\n",
    "    if new_reviews >0 or time_since_scrapping >86400:\n",
    "        reviews_df = scrapper.get_reviews(movie_id, total_reviews)\n",
    "\n",
    "        # Get the text hidden behind spoiler markup\n",
    "        empty_reviews = reviews_df[pd.isnull(reviews_df[\"text\"]) | (reviews_df[\"text\"].str.strip() == \"\")]\n",
    "\n",
    "        if len(empty_reviews) > 0:\n",
    "            print(f\"[WARNING] Missing text for {len(empty_reviews)} reviews\")\n",
    "            print(f\"[INFO] Getting text behind spoiler markups\")\n",
    "    \n",
    "            for index, row in tqdm.tqdm(empty_reviews.iterrows(), total=len(empty_reviews), desc=\"Processing empty reviews\"):\n",
    "                review_id = row[\"review_id\"]\n",
    "                spoiler_text = scrapper.get_spoiler(review_id)  # Call the function to get the spoiler\n",
    "                reviews_df.at[index, \"text\"] = spoiler_text  # Replace 'text' with the spoiler\n",
    "\n",
    "        # Check again for empty reviews\n",
    "        empty_reviews = reviews_df[reviews_df[\"text\"].isna() | reviews_df[\"text\"].str.strip().eq(\"\") |\n",
    "                               reviews_df[\"title\"].isna() | reviews_df[\"title\"].str.strip().eq(\"\")].shape[0]\n",
    "\n",
    "        if empty_reviews > 0:\n",
    "            print(f\"[WARNING] Still missing text or title for {empty_reviews} reviews\")\n",
    "        else:\n",
    "            print(f\"[INFO] No reviews with missing text or title\")\n",
    "\n",
    "        # Get exact vote counts for values >999\n",
    "        print(f\"[INFO] Updating votes\")\n",
    "        mask = reviews_df['upvotes'].astype(str).str.endswith('K') | reviews_df['downvotes'].astype(str).str.endswith('K')\n",
    "        print(f\"[INFO] Found {len(reviews_df[mask])} reviews with rounded votes\")\n",
    "\n",
    "        for index, row in reviews_df[mask].iterrows():\n",
    "            review_id = row['review_id']\n",
    "            exact_upvotes, exact_downvotes = scrapper.get_votes(review_id)\n",
    "            reviews_df.loc[index, 'upvotes'] = exact_upvotes\n",
    "            reviews_df.loc[index, 'downvotes'] = exact_downvotes\n",
    "\n",
    "        reviews_df['upvotes'] = reviews_df['upvotes'].astype(int)\n",
    "        reviews_df['downvotes'] = reviews_df['downvotes'].astype(int)\n",
    "\n",
    "        # Update table\n",
    "        # Create a variable to identify reviews needing sentiment analysis\n",
    "        reviews_df['to_process'] = 1\n",
    "\n",
    "        # Convert data to a list of tuples\n",
    "        reviews_list = reviews_df.apply(lambda row: (\n",
    "            str(row['movie_id']), str(row['review_id']), \n",
    "            str(row['author']), str(row['title']), \n",
    "            str(row['text']), row['rating'],\n",
    "            str(row['date']), row['upvotes'],  \n",
    "            row['downvotes'], row['last_update'], row['to_process']  \n",
    "        ), axis=1).tolist()\n",
    "\n",
    "        # Replace NaN with None to avoid errors with postgreSQL\n",
    "        reviews_list = [tuple(None if pd.isna(x) else x for x in row) for row in reviews_list]\n",
    "\n",
    "        # Upserting\n",
    "        db.upsert_review_data(reviews_list)\n",
    "\n",
    "    # Closing browser\n",
    "    scrapper.close()\n",
    "    print(f\"[INFO] Finished scrapping for movie #{movie_id}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e249a9-0279-4310-9eca-7e64217c98d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "###     SENTIMENT ANALYSIS     ###\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5646a4d-8a11-4192-86dd-141c16f07933",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_to_process = db.query_data('reviews_raw', condition=f\"to_process = 1\")\n",
    "\n",
    "if len(reviews_to_process) == 0:\n",
    "    print(f\"[INFO] No new reviews to analyze\")\n",
    "\n",
    "else:\n",
    "    unirev = len(reviews_to_process)\n",
    "    unimov = len(pd.DataFrame(reviews_to_process)[0].unique())\n",
    "    prompt1 = f\"1 review\" if unirev==1 else f\"{unirev} reviews\"\n",
    "    prompt2 = f\"1 movie\" if unimov==1 else f\"{unimov} movies\"\n",
    "    print(f\"[INFO] {prompt1} to analyze for {prompt2}\")\n",
    "    print(f\"[INFO] Starting API calls...\")\n",
    "\n",
    "    analyzer = GPT()\n",
    "    for review in tqdm.tqdm(reviews_to_process, desc=\"Analyzing reviews sentiment\", unit=\"review\"):\n",
    "        review_id = review[1]\n",
    "        GPT_results = analyzer.sentiment(review)\n",
    "        data = [(review_id, *GPT_results)]\n",
    "        db.update_sentiment_data(data)\n",
    "        db.reset_indicator(review_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2320c296-a20c-47ca-bdbd-816648a2e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "###           BACKUP           ###\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5ade21-d6ac-4f4a-ac9e-62d9d8ac9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring S3\n",
    "S3_ENDPOINT_URL = 'https://' + os.environ['AWS_S3_ENDPOINT']\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "bucket_name = 'maeldieudonne'\n",
    "destination = bucket_name + '/diffusion/'\n",
    "\n",
    "# Save the tables to parquet\n",
    "for table in ['movies', 'reviews_raw', 'reviews_sentiments']:\n",
    "    db.backup_table(table)\n",
    "\n",
    "# Check if other save files are present and select the newest\n",
    "def get_latest_local_backup(table_name):\n",
    "    backup_files = [f for f in os.listdir(\"data/backups\") if f.startswith(table_name)]\n",
    "    \n",
    "    if not backup_files:\n",
    "        print(f\"[INFO] No local backup found for {table_name}\")\n",
    "        return None\n",
    "\n",
    "    else:\n",
    "        latest_backup = max(backup_files, key=lambda f: os.path.getctime(os.path.join(\"data/backups\", f)))\n",
    "        file_path = os.path.join(\"data/backups\", latest_backup)\n",
    "        return file_path\n",
    "\n",
    "# Upload the files to S3\n",
    "for table in ['movies', 'reviews_raw', 'reviews_sentiments']:   \n",
    "    file_path = get_latest_local_backup(table)\n",
    "        \n",
    "    if file_path is not None:\n",
    "        try:\n",
    "            fs.put(file_path, destination, content_type=\"parquet\", encoding=\"utf-8\")\n",
    "            os.remove(file_path)\n",
    "            print(f\"[INFO] Successfully uploaded {file_path} to {destination}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed uploading {file_path} to {destination}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18b597-e042-4162-b76d-8f73adee33cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close_connection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
